{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGvYYjysXMau",
        "outputId": "21604363-6a90-4d5c-de22-41cd3246af68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from torch.utils.data import Dataset\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class LicensePlateDataset(Dataset):\n",
        "    def __init__(self, tokenizer, data_dir=\"./data\", max_samples=100):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data_dir = data_dir\n",
        "        self.max_samples = max_samples\n",
        "        self.data = self._load_data()\n",
        "\n",
        "    def _load_data(self):\n",
        "        processed_data = []\n",
        "\n",
        "        # Process each data file with controlled sampling\n",
        "        for file_name, status in [\n",
        "            ('accepted-plates.csv', 'ACCEPTED'),\n",
        "            ('rejected-plates.csv', 'REJECTED'),\n",
        "            ('red-guide.csv', 'GUIDE')\n",
        "        ]:\n",
        "            try:\n",
        "                file_path = os.path.join(self.data_dir, file_name)\n",
        "                if os.path.exists(file_path):\n",
        "                    df = pd.read_csv(file_path)\n",
        "                    # Use random sampling instead of head()\n",
        "                    if len(df) > self.max_samples:\n",
        "                        df = df.sample(n=self.max_samples, random_state=42)\n",
        "\n",
        "                    for plate in df['plate'].dropna():\n",
        "                        processed_data.append({\n",
        "                            'plate': str(plate).upper().strip(),\n",
        "                            'status': status\n",
        "                        })\n",
        "                    print(f\"Successfully processed {len(df)} samples from {file_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_name}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        # Create a simple, standardized input format\n",
        "        input_text = f\"License plate analysis: {item['plate']}\"\n",
        "        target_text = f\" Classification: {item['status']}\"\n",
        "\n",
        "        # Tokenize input separately\n",
        "        input_encoding = self.tokenizer(\n",
        "            input_text,\n",
        "            truncation=True,\n",
        "            max_length=32,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Tokenize full sequence for labels\n",
        "        full_encoding = self.tokenizer(\n",
        "            input_text + target_text,\n",
        "            truncation=True,\n",
        "            max_length=32,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Create labels with -100 for input tokens\n",
        "        labels = full_encoding['input_ids'].clone()\n",
        "        labels[:, :input_encoding['input_ids'].shape[1]] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': input_encoding['attention_mask'].squeeze(),\n",
        "            'labels': labels.squeeze()\n",
        "        }\n",
        "\n",
        "def train_model():\n",
        "    print(\"Initializing optimized training process...\")\n",
        "\n",
        "    # Clear any existing cached memory\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Initialize model and tokenizer\n",
        "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load model with optimized settings\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float32,  # Use full precision initially\n",
        "        low_cpu_mem_usage=True,\n",
        "        use_cache=False  # Disable cache for gradient checkpointing\n",
        "    )\n",
        "\n",
        "    # Create dataset with controlled size\n",
        "    dataset = LicensePlateDataset(tokenizer, max_samples=100)\n",
        "\n",
        "    # Configure training arguments for stability\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"trained_model\",\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=8,\n",
        "        logging_steps=10,\n",
        "        save_steps=100,\n",
        "        learning_rate=5e-5,\n",
        "        warmup_steps=10,\n",
        "        weight_decay=0.01,\n",
        "        save_total_limit=1,\n",
        "        report_to=\"none\",\n",
        "        evaluation_strategy=\"no\",\n",
        "        save_strategy=\"epoch\",\n",
        "        fp16=False,  # Disable FP16 training\n",
        "        gradient_checkpointing=True,\n",
        "        dataloader_num_workers=0,\n",
        "        remove_unused_columns=False\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"Saving model...\")\n",
        "    trainer.save_model(\"trained_model\")\n",
        "    tokenizer.save_pretrained(\"trained_model\")\n",
        "    print(\"Training completed successfully!\")\n",
        "\n",
        "    return \"trained_model\"\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        print(\"Checking GPU memory...\")\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"GPU Memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "        trained_model_path = train_model()\n",
        "        print(f\"Model saved to: {trained_model_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    finally:\n",
        "        # Clean up memory\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "G15IfHH_BYed",
        "outputId": "b3a4fb14-0534-4325-df53-5cd6365db6ae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking GPU memory...\n",
            "GPU Memory available: 42.48 GB\n",
            "Initializing optimized training process...\n",
            "Successfully processed 100 samples from accepted-plates.csv\n",
            "Successfully processed 100 samples from rejected-plates.csv\n",
            "Successfully processed 100 samples from red-guide.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [18/18 01:40, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model...\n",
            "Training completed successfully!\n",
            "Model saved to: trained_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import logging\n",
        "from typing import Dict\n",
        "import gc\n",
        "\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class SavedModelAnalyzer:\n",
        "    def __init__(self, model_path=\"trained_model\"):\n",
        "        \"\"\"Initialize analyzer with saved model.\"\"\"\n",
        "        self.model_path = model_path\n",
        "        self.setup_model()\n",
        "\n",
        "    def setup_model(self):\n",
        "        \"\"\"Load the saved TinyLlama model.\"\"\"\n",
        "        logger.info(\"Loading saved model and tokenizer...\")\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_path,\n",
        "                torch_dtype=torch.float32,\n",
        "                low_cpu_mem_usage=True,\n",
        "                device_map=\"auto\"\n",
        "            )\n",
        "            logger.info(\"Model loaded successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def free_memory(self):\n",
        "        \"\"\"Clean up memory.\"\"\"\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def analyze_plate(self, plate: str) -> Dict[str, str]:\n",
        "        \"\"\"Generate plate analysis using saved model.\"\"\"\n",
        "        prompt = f\"\"\"<human>: Analyze this license plate: {plate}\n",
        "Consider these aspects:\n",
        "1. Is it personalized or random?\n",
        "2. Is it appropriate/legal?\n",
        "3. What does it mean?\n",
        "4. What category (personal, hobby, sports, etc.)?\n",
        "Provide a brief but complete analysis.\n",
        "\n",
        "<assistant>: I'll analyze the license plate '{plate}'.\"\"\"\n",
        "\n",
        "        try:\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=128\n",
        "            ).to(self.model.device)\n",
        "\n",
        "            with torch.inference_mode():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs[\"input_ids\"],\n",
        "                    max_length=256,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9,\n",
        "                    do_sample=True,\n",
        "                    num_beams=1,\n",
        "                    early_stopping=True,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id\n",
        "                )\n",
        "\n",
        "            analysis = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            analysis = analysis.split(\"<assistant>:\")[-1].strip()\n",
        "            self.free_memory()\n",
        "\n",
        "            return {\n",
        "                'analysis': analysis,\n",
        "                'plate': plate\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error analyzing plate {plate}: {str(e)}\")\n",
        "            return {'error': str(e), 'plate': plate}\n",
        "\n",
        "def analyze_plates_interactive():\n",
        "    \"\"\"Interactive analysis function using saved model.\"\"\"\n",
        "    try:\n",
        "        print(\"\\nInitializing License Plate Analyzer...\")\n",
        "        print(\"Loading saved model (this may take a moment)...\")\n",
        "        analyzer = SavedModelAnalyzer()\n",
        "        print(\"\\nAnalyzer ready!\")\n",
        "\n",
        "        while True:\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"License Plate Analysis Tool\")\n",
        "            print(\"=\"*50)\n",
        "            print(\"\\nOptions:\")\n",
        "            print(\"1. Enter a license plate to analyze\")\n",
        "            print(\"2. Exit\")\n",
        "\n",
        "            choice = input(\"\\nEnter your choice (1 or 2): \").strip()\n",
        "\n",
        "            if choice == '2':\n",
        "                print(\"\\nExiting analyzer. Goodbye!\")\n",
        "                break\n",
        "\n",
        "            elif choice == '1':\n",
        "                plate = input(\"\\nEnter license plate to analyze: \").strip().upper()\n",
        "\n",
        "                if not plate:\n",
        "                    print(\"Please enter a valid plate number.\")\n",
        "                    continue\n",
        "\n",
        "                print(\"\\nAnalyzing plate:\", plate)\n",
        "                print(\"-\" * 50)\n",
        "\n",
        "                result = analyzer.analyze_plate(plate)\n",
        "\n",
        "                if 'error' in result:\n",
        "                    print(f\"Error analyzing plate: {result['error']}\")\n",
        "                else:\n",
        "                    print(\"\\nAnalysis Results:\")\n",
        "                    print(result['analysis'])\n",
        "\n",
        "                input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "            else:\n",
        "                print(\"\\nInvalid choice. Please try again.\")\n",
        "\n",
        "            analyzer.free_memory()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred: {str(e)}\")\n",
        "        logger.error(f\"Analysis error: {str(e)}\")\n",
        "\n",
        "    finally:\n",
        "        print(\"\\nCleaning up resources...\")\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    analyze_plates_interactive()"
      ],
      "metadata": {
        "id": "6FwX5MCXFLYB",
        "outputId": "faa1f49e-385d-4110-fa7f-d8935a96046e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing License Plate Analyzer...\n",
            "Loading saved model (this may take a moment)...\n",
            "\n",
            "Analyzer ready!\n",
            "\n",
            "==================================================\n",
            "License Plate Analysis Tool\n",
            "==================================================\n",
            "\n",
            "Options:\n",
            "1. Enter a license plate to analyze\n",
            "2. Exit\n",
            "\n",
            "Enter your choice (1 or 2): 1\n",
            "\n",
            "Enter license plate to analyze: vishnu\n",
            "\n",
            "Analyzing plate: VISHNU\n",
            "--------------------------------------------------\n",
            "\n",
            "Analysis Results:\n",
            "I'll analyze the license plate 'VISHNU'.\n",
            "\n",
            "1. Is it personalized or random?\n",
            "\n",
            "The license plate 'VISHNU' is personalized, meaning that the license plate number is the same as the driver's name. This is a common practice among Indian drivers to identify themselves by their name.\n",
            "\n",
            "2. Is it appropriate/legal?\n",
            "\n",
            "The license plate 'VISHNU' is legal, meaning that it follows the Indian Motor Vehicles Act of 1989. The license plate designates a vehicle as a Vishnu vehicle, which is a religious symbol in Hinduism.\n",
            "\n",
            "3. What does it mean?\n",
            "\n",
            "The license plate 'VISHNU' means that the vehicle is owned by Vishnu, a Hindu god.\n",
            "\n",
            "4. What category\n"
          ]
        }
      ]
    }
  ]
}